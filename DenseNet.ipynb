{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNq/XBQGCrqlI4Ser3U0+eM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/macorony/Convolutional-Network/blob/main/DenseNet.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Understanding DenseNet: Architecture and Advantages\n",
        "DenseNet, short for Dense Convolutional Network, is a CNN architecture known for its unique connectivity pattern, which aims to maximize information flow between layers. Unlike traditional architectures where each layer connects only to its subsequent layer, DenseNet directly connects each layer to every other layer in a feed-forward fashion.\n",
        "\n",
        "## Dense Connectivity: Fostering Feature Reuse and Information Flow\n",
        "\n",
        "1. In a DenseNet with L layers, there are L(L+1)/2 direct connections. Each layer receives the feature maps from all preceding layers as input, and its own feature maps are passed as input to all subsequent layers.\n",
        "\n",
        "2. This dense connectivity offers several advantages:\n",
        "  *  Alleviates the vanishing gradient problem: Direct connections to the loss function and original input signal provide each layer with clear gradients, facilitating training of deeper networks. This is similar to the concept of \"deep supervision\".\n",
        "  \n",
        "  *  Strengthens feature propagation: Information flows more efficiently throughout the network due to the short paths created by dense connections.\n",
        "  \n",
        "  *  Encourages feature reuse: Layers have access to features learned at different levels of the network, promoting the learning of compact and efficient representations.\n",
        "\n",
        "## Structure of DenseNet: Dense Blocks and Transition Layers\n",
        "DenseNets are typically organized into dense blocks interconnected by transition layers.\n",
        "* Dense Block: Within a dense block, layers are densely connected as described above. Each layer adds a small set of feature maps (k feature maps, where k is the growth rate) to the network's \"collective knowledge\".\n",
        "\n",
        "* Transition Layer: Transition layers are placed between dense blocks to change feature map sizes via convolution and pooling. They help to control the network's complexity and reduce the number of feature maps.\n",
        "  * Compression: To further reduce the number of parameters, DenseNet-C employs a compression factor (θ) in transition layers to reduce the number of output feature maps.\n",
        "  \n",
        "  * Bottleneck Layers: DenseNet-BC introduces 1x1 convolutions (bottleneck layers) before each 3x3 convolution in dense blocks to further reduce computational complexity.\n",
        "\n",
        "## Key Features and Design Choices:\n",
        "* Growth rate (k): This hyperparameter controls the number of feature maps added by each layer. A smaller growth rate results in narrower layers, leading to a more compact model.\n",
        "\n",
        "* Bottleneck and Compression: These techniques aim to improve model compactness and efficiency. DenseNet-BC combines both bottleneck layers and compression in transition layers.\n",
        "\n",
        "* Composite Function: Each layer's transformation in DenseNet often consists of batch normalization, ReLU activation, and a 3x3 convolution.\n",
        "Performance and Efficiency:\n",
        "\n",
        "* Experiments show that DenseNets can achieve high accuracy with fewer parameters compared to other architectures, particularly ResNets.\n",
        "\n",
        "* The dense connectivity pattern enables efficient feature reuse and information flow, contributing to better performance and easier training.\n",
        "\n",
        "* Memory-efficient implementations are important for practical use, especially on GPUs.\n",
        "\n",
        "## Connection to Other Architectures:\n",
        "* ResNets: DenseNet draws inspiration from ResNets' use of skip connections to improve information flow. However, DenseNet concatenates features instead of summing them, potentially mitigating information loss.\n",
        "\n",
        "* Stochastic Depth: There's an intriguing link between DenseNet and stochastic depth regularization in ResNets, as both create direct connections between non-adjacent layers.\n",
        "\n",
        "* Inception Networks: Like Inception networks, DenseNets concatenate features from different layers, but they are simpler and more efficient.\n",
        "\n",
        "\n",
        "## Conclusion:\n",
        "DenseNet offers a compelling approach to CNN design, emphasizing feature reuse and efficient information flow through dense connectivity. Its ability to achieve high accuracy with fewer parameters makes it an attractive choice for various computer vision tasks.\n",
        "\n",
        "\n",
        "***resource paper***: https://arxiv.org/abs/1608.06993v5"
      ],
      "metadata": {
        "id": "ioEsbz8DzTwE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Implementation"
      ],
      "metadata": {
        "id": "XRDkDqhq_kRB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ],
      "metadata": {
        "id": "jbPtexmFzTI1"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransitionLayer(nn.Module):\n",
        "  def __init__(self, in_channels, out_channels):\n",
        "    super(TransitionLayer, self).__init__()\n",
        "    self.transition = nn.Sequential(\n",
        "        nn.BatchNorm2d(in_channels),\n",
        "        nn.ReLU(inplace=True),\n",
        "        nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, biase=False),\n",
        "        nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "    )\n",
        "  def forward(self, x):\n",
        "    return self.transition(x)"
      ],
      "metadata": {
        "id": "kM37N3sg_qi3"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-activation\n",
        "\n",
        "The order of operations in the transition layer (BN → ReLU → Conv rather than Conv → BN → ReLU) follows a principle called \"pre-activation\" that was introduced in the paper \"Identity Mappings in Deep Residual Networks\" by He et al.\n",
        "Here are the key reasons:\n",
        "\n",
        "* Better Gradient Flow\n",
        "\n",
        "  * When the activation (ReLU) comes before the convolution, it helps prevent the gradients from vanishing during backpropagation\n",
        "  * The pre-activation ensures that the gradient can flow through the network without being diminished by dying ReLU units\n",
        "\n",
        "* Better Regularization\n",
        "  * Batch normalization before ReLU helps normalize the input distribution before applying the non-linearity\n",
        "  * This makes the optimization landscape smoother and helps with training\n",
        "\n",
        "* Improved Feature Refinement\n",
        "  * The normalized and activated features are then processed by the convolution layer\n",
        "  * This means the convolution layer receives cleaner, more stable inputs to work with"
      ],
      "metadata": {
        "id": "IqkQnsYSDUb5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "o8O2kcYhC8Kh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}